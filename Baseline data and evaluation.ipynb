{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0ab785b",
   "metadata": {},
   "source": [
    "### Sample subset of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fb368c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "\n",
    "# Load full dataset\n",
    "ds = load_dataset(\"google/MusicCaps\")[\"train\"]\n",
    "\n",
    "# Shuffle and sample\n",
    "sampled = ds.shuffle(seed=42).select(range(120))  # pick 100 random entries\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(sampled)\n",
    "\n",
    "# Save to CSV\n",
    "df[[\"ytid\", \"caption\"]].to_csv(\"sampled_musiccaps.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b31087d",
   "metadata": {},
   "source": [
    "### Fetch audio clips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5ad925",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import os\n",
    "# import subprocess\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# # Load the sample\n",
    "# df = pd.read_csv(\"sampled_musiccaps.csv\")\n",
    "# output_dir = \"clips\"\n",
    "# os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# def download_clip(ytid, output_dir):\n",
    "#     url = f\"https://www.youtube.com/watch?v={ytid}\"\n",
    "#     output_path = os.path.join(output_dir, f\"{ytid}.mp3\")\n",
    "\n",
    "#     if os.path.exists(output_path):\n",
    "#         return\n",
    "\n",
    "#     command = [\n",
    "#         \"yt-dlp\",\n",
    "#         \"-x\", \"--audio-format\", \"mp3\",\n",
    "#         \"--postprocessor-args\", \"-ss 00:00:00 -t 10\",  # first 10 seconds\n",
    "#         url,\n",
    "#         \"-o\", os.path.join(output_dir, f\"{ytid}.%(ext)s\")\n",
    "#     ]\n",
    "    \n",
    "#     try:\n",
    "#         subprocess.run(command, check=True)\n",
    "#     except subprocess.CalledProcessError as e:\n",
    "#         print(f\"Failed to download {ytid}: {e}\")\n",
    "\n",
    "# # Download all clips\n",
    "# for ytid in tqdm(df[\"ytid\"]):\n",
    "#     download_clip(ytid, output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "65161fec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Attempting to download 100 valid clips...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 105/120 [00:24<00:03,  4.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Successfully downloaded 100 audio clips.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ---- Config ----\n",
    "input_csv = \"sampled_musiccaps.csv\"\n",
    "output_csv = \"metadata_100.csv\"\n",
    "output_dir = \"clips\"\n",
    "target_count = 100\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# ---- Load full metadata ----\n",
    "df = pd.read_csv(input_csv)\n",
    "valid_rows = []\n",
    "\n",
    "def download_10s_clip(ytid, out_dir):\n",
    "    url = f\"https://www.youtube.com/watch?v={ytid}\"\n",
    "    out_path = os.path.join(out_dir, f\"{ytid}.mp3\")\n",
    "    if os.path.exists(out_path):\n",
    "        return True\n",
    "\n",
    "    command = [\n",
    "        \"yt-dlp\",\n",
    "        \"-x\", \"--audio-format\", \"mp3\",\n",
    "        \"--postprocessor-args\", \"-ss 00:00:00 -t 10\",\n",
    "        url,\n",
    "        \"-o\", os.path.join(out_dir, f\"{ytid}.%(ext)s\")\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        subprocess.run(command, check=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
    "        return os.path.exists(out_path)\n",
    "    except subprocess.CalledProcessError:\n",
    "        return False\n",
    "\n",
    "# ---- Try downloading until 100 clips are available ----\n",
    "print(f\"üöÄ Attempting to download 100 valid clips...\")\n",
    "for _, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    if len(valid_rows) >= target_count:\n",
    "        break\n",
    "    ytid = row[\"ytid\"]\n",
    "    success = download_10s_clip(ytid, output_dir)\n",
    "    if success:\n",
    "        valid_rows.append(row)\n",
    "\n",
    "# ---- Save updated metadata ----\n",
    "print(f\"‚úÖ Successfully downloaded {len(valid_rows)} audio clips.\")\n",
    "pd.DataFrame(valid_rows).to_csv(output_csv, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e67f7d",
   "metadata": {},
   "source": [
    "### Compute CLAP Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "50d5d57a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:29<00:00,  3.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Average CLAP similarity across 100 examples: 0.4410\n",
      "‚úÖ CLAP similarity computation complete. Results saved to: clap_similarity_results.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torchaudio\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import ClapProcessor, ClapModel\n",
    "from scipy.spatial.distance import cosine\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "# -------------------------\n",
    "# Config\n",
    "# -------------------------\n",
    "CLIP_DIR = \"clips\"  # where your downloaded 10s audio clips are\n",
    "SAMPLE_RATE = 48000  # CLAP expects 48kHz\n",
    "OUTPUT_CSV = \"clap_similarity_results.csv\"\n",
    "\n",
    "# -------------------------\n",
    "# Load CLAP\n",
    "# -------------------------\n",
    "clap_model = ClapModel.from_pretrained(\"laion/clap-htsat-unfused\")\n",
    "clap_processor = ClapProcessor.from_pretrained(\"laion/clap-htsat-unfused\")\n",
    "\n",
    "# -------------------------\n",
    "# Load Metadata\n",
    "# -------------------------\n",
    "df = pd.read_csv(\"metadata_100.csv\")  # should have columns: ytid, caption\n",
    "\n",
    "# -------------------------\n",
    "# CLAP Similarity Computation\n",
    "# -------------------------\n",
    "results = []\n",
    "\n",
    "for _, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    ytid = row[\"ytid\"]\n",
    "    caption = row[\"caption\"]\n",
    "    audio_path = None\n",
    "\n",
    "    # Try both .mp3 and .wav\n",
    "    for ext in [\".mp3\", \".wav\"]:\n",
    "        possible_path = os.path.join(CLIP_DIR, f\"{ytid}{ext}\")\n",
    "        if os.path.exists(possible_path):\n",
    "            audio_path = possible_path\n",
    "            break\n",
    "\n",
    "    if audio_path is None:\n",
    "        print(f\"‚ö†Ô∏è Audio not found for {ytid}, skipping.\")\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        # Load and resample to 48kHz\n",
    "        waveform, sr = torchaudio.load(audio_path)\n",
    "\n",
    "        # Resample if needed\n",
    "        if sr != SAMPLE_RATE:\n",
    "            waveform = torchaudio.functional.resample(waveform, orig_freq=sr, new_freq=SAMPLE_RATE)\n",
    "\n",
    "        # Convert to mono if stereo\n",
    "        if waveform.ndim == 2 and waveform.shape[0] > 1:\n",
    "            waveform = waveform.mean(dim=0, keepdim=True)\n",
    "\n",
    "        # Make sure it's shape (1, N)\n",
    "        if waveform.ndim == 1:\n",
    "            waveform = waveform.unsqueeze(0)\n",
    "\n",
    "        # Process for CLAP\n",
    "        waveform_np = waveform.squeeze().numpy() \n",
    "        audio_inputs = clap_processor(audios=waveform_np, return_tensors=\"pt\", sampling_rate=SAMPLE_RATE)\n",
    "        text_inputs = clap_processor(text=[caption], return_tensors=\"pt\")\n",
    "\n",
    "        with torch.no_grad():\n",
    "            audio_embed = clap_model.get_audio_features(**audio_inputs).numpy()[0]\n",
    "            text_embed = clap_model.get_text_features(**text_inputs).numpy()[0]\n",
    "\n",
    "        similarity = 1 - cosine(audio_embed, text_embed)\n",
    "        results.append({\"ytid\": ytid, \"caption\": caption, \"clap_similarity\": similarity})\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error processing {ytid}: {e}\")\n",
    "        continue\n",
    "\n",
    "# -------------------------\n",
    "# Save Results\n",
    "# -------------------------\n",
    "if results:\n",
    "    average_clap = np.mean([r['clap_similarity'] for r in results])\n",
    "    print(f\"üìä Average CLAP similarity across {len(results)} examples: {average_clap:.4f}\")\n",
    "    pd.DataFrame(results).to_csv(OUTPUT_CSV, index=False)\n",
    "    print(\"‚úÖ CLAP similarity computation complete. Results saved to:\", OUTPUT_CSV)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No valid audio processed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98238134",
   "metadata": {},
   "source": [
    "### Musicgen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb60a5a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîä Loading MusicGen model: facebook/musicgen-large\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tidris/miniconda3/envs/musicgen/lib/python3.9/site-packages/torch/nn/utils/weight_norm.py:30: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéº Loading CLAP model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [33:40<00:00, 20.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Avg CLAP similarity across 100 examples: 0.4225\n",
      "‚úÖ Done. Results saved to: clap_similarity_results_2.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from transformers import ClapProcessor, ClapModel\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "from audiocraft.models import MusicGen, MultiBandDiffusion\n",
    "\n",
    "# -------------------------\n",
    "# CONFIG\n",
    "# -------------------------\n",
    "MODEL_SIZE = 'facebook/musicgen-large'  # Options: musicgen-small, medium, large\n",
    "USE_DIFFUSION_DECODER = False           # Set True if using MultiBandDiffusion\n",
    "GEN_DURATION = 10                       # seconds\n",
    "TOP_K = 250\n",
    "SAMPLE_RATE = 48000                     # Required for CLAP\n",
    "OUTPUT_DIR = 'generated_clap_eval'\n",
    "CSV_INPUT = 'metadata_100.csv'          # Requires 'ytid' and 'caption' columns\n",
    "CSV_OUTPUT = 'clap_similarity_results_2.csv'\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# -------------------------\n",
    "# LOAD MODELS\n",
    "# -------------------------\n",
    "print(f\"üîä Loading MusicGen model: {MODEL_SIZE}\")\n",
    "model = MusicGen.get_pretrained(MODEL_SIZE)\n",
    "model.set_generation_params(use_sampling=True, top_k=TOP_K, duration=GEN_DURATION)\n",
    "\n",
    "mbd = None\n",
    "if USE_DIFFUSION_DECODER:\n",
    "    print(\"üéß Loading MultiBandDiffusion...\")\n",
    "    mbd = MultiBandDiffusion.get_mbd_musicgen()\n",
    "\n",
    "print(\"üéº Loading CLAP model...\")\n",
    "clap_model = ClapModel.from_pretrained(\"laion/clap-htsat-unfused\")\n",
    "clap_processor = ClapProcessor.from_pretrained(\"laion/clap-htsat-unfused\")\n",
    "\n",
    "# -------------------------\n",
    "# LOAD DATASET\n",
    "# -------------------------\n",
    "df = pd.read_csv(CSV_INPUT)\n",
    "\n",
    "# -------------------------\n",
    "# GENERATE + CLAP LOOP\n",
    "# -------------------------\n",
    "results = []\n",
    "\n",
    "for _, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    ytid = row[\"ytid\"]\n",
    "    prompt = row[\"caption\"]\n",
    "\n",
    "    try:\n",
    "        # --- MusicGen Generation ---\n",
    "        output = model.generate([prompt], progress=False, return_tokens=USE_DIFFUSION_DECODER)\n",
    "        wav = output[0].cpu()\n",
    "\n",
    "        if USE_DIFFUSION_DECODER:\n",
    "            wav = mbd.tokens_to_wav(output[1])[0].cpu()\n",
    "\n",
    "        # Save audio\n",
    "        out_path = os.path.join(OUTPUT_DIR, f\"{ytid}.wav\")\n",
    "        torchaudio.save(out_path, wav, SAMPLE_RATE)\n",
    "\n",
    "        # --- CLAP Similarity ---\n",
    "        if wav.shape[0] > 1:\n",
    "            wav = wav.mean(dim=0, keepdim=True)\n",
    "        wav_np = wav.squeeze().numpy()\n",
    "\n",
    "        audio_inputs = clap_processor(audios=wav_np, return_tensors=\"pt\", sampling_rate=SAMPLE_RATE)\n",
    "        text_inputs = clap_processor(text=[prompt], return_tensors=\"pt\")\n",
    "\n",
    "        with torch.no_grad():\n",
    "            audio_embed = clap_model.get_audio_features(**audio_inputs).numpy()[0]\n",
    "            text_embed = clap_model.get_text_features(**text_inputs).numpy()[0]\n",
    "\n",
    "        similarity = 1 - cosine(audio_embed, text_embed)\n",
    "        results.append({\n",
    "            \"ytid\": ytid,\n",
    "            \"caption\": prompt,\n",
    "            \"clap_similarity\": similarity\n",
    "        })\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error with {ytid}: {e}\")\n",
    "        continue\n",
    "\n",
    "# -------------------------\n",
    "# SAVE RESULTS\n",
    "# -------------------------\n",
    "if results:\n",
    "    avg_sim = np.mean([r['clap_similarity'] for r in results])\n",
    "    print(f\"üìä Avg CLAP similarity across {len(results)} examples: {avg_sim:.4f}\")\n",
    "    pd.DataFrame(results).to_csv(CSV_OUTPUT, index=False)\n",
    "    print(f\"‚úÖ Done. Results saved to: {CSV_OUTPUT}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No results generated.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d1f1abd",
   "metadata": {},
   "source": [
    "### Baseline for our curated dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb6c9719",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîä Loading MusicGen model: facebook/musicgen-large\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tidris/miniconda3/envs/musicgen/lib/python3.9/site-packages/torch/nn/utils/weight_norm.py:30: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéº Loading CLAP model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 38/38 [12:45<00:00, 20.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Avg CLAP similarity across 38 examples: 0.4404\n",
      "‚úÖ Done. Results saved to: clap_similarity_results_curated.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from transformers import ClapProcessor, ClapModel\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "from audiocraft.models import MusicGen, MultiBandDiffusion\n",
    "\n",
    "# -------------------------\n",
    "# CONFIG\n",
    "# -------------------------\n",
    "MODEL_SIZE = 'facebook/musicgen-large'  # Options: musicgen-small, medium, large\n",
    "USE_DIFFUSION_DECODER = False           # Set True if using MultiBandDiffusion\n",
    "GEN_DURATION = 10                       # seconds\n",
    "TOP_K = 250\n",
    "SAMPLE_RATE = 48000                     # Required for CLAP\n",
    "OUTPUT_DIR = 'generated_clap_eval_curated'\n",
    "CSV_INPUT = 'prompt_template.csv'          # Requires 'ytid' and 'caption' columns\n",
    "CSV_OUTPUT = 'clap_similarity_results_curated.csv'\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# -------------------------\n",
    "# LOAD MODELS\n",
    "# -------------------------\n",
    "print(f\"üîä Loading MusicGen model: {MODEL_SIZE}\")\n",
    "model = MusicGen.get_pretrained(MODEL_SIZE)\n",
    "model.set_generation_params(use_sampling=True, top_k=TOP_K, duration=GEN_DURATION)\n",
    "\n",
    "mbd = None\n",
    "if USE_DIFFUSION_DECODER:\n",
    "    print(\"üéß Loading MultiBandDiffusion...\")\n",
    "    mbd = MultiBandDiffusion.get_mbd_musicgen()\n",
    "\n",
    "print(\"üéº Loading CLAP model...\")\n",
    "clap_model = ClapModel.from_pretrained(\"laion/clap-htsat-unfused\")\n",
    "clap_processor = ClapProcessor.from_pretrained(\"laion/clap-htsat-unfused\")\n",
    "\n",
    "# -------------------------\n",
    "# LOAD DATASET\n",
    "# -------------------------\n",
    "df = pd.read_csv(CSV_INPUT)\n",
    "\n",
    "# -------------------------\n",
    "# GENERATE + CLAP LOOP\n",
    "# -------------------------\n",
    "results = []\n",
    "\n",
    "for _, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    audio = row[\"audio\"]\n",
    "    prompt = row[\"prompt\"]\n",
    "\n",
    "    try:\n",
    "        # --- MusicGen Generation ---\n",
    "        output = model.generate([prompt], progress=False, return_tokens=USE_DIFFUSION_DECODER)\n",
    "        wav = output[0].cpu()\n",
    "\n",
    "        if USE_DIFFUSION_DECODER:\n",
    "            wav = mbd.tokens_to_wav(output[1])[0].cpu()\n",
    "\n",
    "        # Save audio\n",
    "        out_path = os.path.join(OUTPUT_DIR, f\"{audio}.wav\")\n",
    "        torchaudio.save(out_path, wav, SAMPLE_RATE)\n",
    "\n",
    "        # --- CLAP Similarity ---\n",
    "        if wav.shape[0] > 1:\n",
    "            wav = wav.mean(dim=0, keepdim=True)\n",
    "        wav_np = wav.squeeze().numpy()\n",
    "\n",
    "        audio_inputs = clap_processor(audios=wav_np, return_tensors=\"pt\", sampling_rate=SAMPLE_RATE)\n",
    "        text_inputs = clap_processor(text=[prompt], return_tensors=\"pt\")\n",
    "\n",
    "        with torch.no_grad():\n",
    "            audio_embed = clap_model.get_audio_features(**audio_inputs).numpy()[0]\n",
    "            text_embed = clap_model.get_text_features(**text_inputs).numpy()[0]\n",
    "\n",
    "        similarity = 1 - cosine(audio_embed, text_embed)\n",
    "        results.append({\n",
    "            \"audio\": audio,\n",
    "            \"caption\": prompt,\n",
    "            \"clap_similarity\": similarity\n",
    "        })\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error with {audio}: {e}\")\n",
    "        continue\n",
    "\n",
    "# -------------------------\n",
    "# SAVE RESULTS\n",
    "# -------------------------\n",
    "if results:\n",
    "    avg_sim = np.mean([r['clap_similarity'] for r in results])\n",
    "    print(f\"üìä Avg CLAP similarity across {len(results)} examples: {avg_sim:.4f}\")\n",
    "    pd.DataFrame(results).to_csv(CSV_OUTPUT, index=False)\n",
    "    print(f\"‚úÖ Done. Results saved to: {CSV_OUTPUT}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No results generated.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07eff51",
   "metadata": {},
   "source": [
    "## Training with (MusicGen Frozen - Update the embedding layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567990ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "musicgen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
