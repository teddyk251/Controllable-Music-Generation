# Compositional Music Generation using Textual Inversion

This repository explores Textual Inversion (TI) for guiding music generation using MusicGen. We inject learned genre-specific tokens into MusicGen's language model (LM) embedding space and analyze their effect on controllability and alignment using CLAP similarity metrics.

## Project Report

ðŸ“„ **[Compositional Textual Inversion for Controllable Music Generation](./docs/Compositional%20Textual%20Inversion%20for%20Controllable%20Music%20Generation.pdf)** - Detailed technical report covering methodology, experiments, and results.

### Project Overview

We propose a compositional textual inversion framework that learns new token embeddings (<lofi>, <jazz>, etc.) from short audio clips. These tokens can be inserted into music prompts to guide MusicGen to generate music in specific genres. We compare the results with baseline generations that do not use TI.


### Setup

1. Clone and Install

```
git clone https://github.com/teddyk251/Controllable-Music-Generation.git
cd Controllable-Music-Generation
conda create -n musicgen python=3.10
conda activate musicgen
pip install -r requirements.txt
```

2. Install FFMPEG (if not already)

```sudo apt-get install ffmpeg  # or use homebrew on Mac```

### Directory Structure

<pre><code>
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ train.py           # Train TI tokens per genre
â”‚   â”œâ”€â”€ inference.py       # Generate music with learned tokens
â”‚   â”œâ”€â”€ evaluate.py        # CLAP similarity evaluation
â”‚   â”œâ”€â”€ app.py             # Gradio interface for generation
â”œâ”€â”€ trained_tokens/        # Learned embeddings per genre
â”œâ”€â”€ preprocessed_data_2/   # Audio clips for training (150s)
â”œâ”€â”€ gradio_generated/      # Generated audio clips via Gradio
â”œâ”€â”€ wandb/                 # Experiment logs (if W&B used)
â”œâ”€â”€ docs/                  # Project documentation
â”‚   â””â”€â”€ Compositional Textual Inversion for Controllable Music Generation.pdf
â”œâ”€â”€ results/
â”‚   â”œâ”€â”€ clap_scores.json   # Evaluation results
â”‚   â””â”€â”€ similarity_plot.png
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
</code></pre>



### Training

#### Train TI Tokens for All Genres

```python scripts/train.py```

	â€¢	Learns one embedding per codebook (4 total) per genre.
	â€¢	Stores them in trained_tokens/{genre}/{genre}_codebook{i}.pt
	â€¢	Logs training loss and learning rate via Weights & Biases

#### Configuration

Modify these in train.py:
```
DATA_DIR = "preprocessed_data_2"
NUM_EPOCHS = 600
LEARNING_RATE = 1e-4
WANDB_PROJECT = "musicgen-textual-inversion_150s"
```





### Inference

Generate music using a trained TI token:

```python scripts/inference.py```

Edit these in inference.py:

```GENRE_NAME = "afrobeat"
LEARNED_TOKEN = "<afrobeat>"
PROMPT = f"A soulful {LEARNED_TOKEN} beat with vibrant percussion"
```

Output is saved to generated_{GENRE_NAME}.wav.



### Evaluation

Evaluate CLAP similarity between generated audio and text prompts:

```python scripts/evaluate.py```

	â€¢	Generates both baseline (natural prompt) and TI (injected token) music.
	â€¢	Computes CLAP similarity against the text prompt.
	â€¢	Outputs scores and bar chart to results/similarity_plot.png.



### Gradio App (Interactive Demo)

Launch an interactive app:

```python scripts/multiselect_gradio.py```

	â€¢	Input your prompt
	â€¢	Select whether to use TI
	â€¢	Choose a token if TI is enabled (You can choose multiply styles to compose a mix of different styles)
	â€¢	Audio preview with playback



### Experimental Highlights

| Genre     | TI Prompt Example                                | Key Observations                            |
|-----------|--------------------------------------------------|---------------------------------------------|
| lofi      | A mellow `<lofi>` beat with nostalgic textures   | TI slightly improves mood alignment         |
| jazz      | A soulful `<jazz>` track with saxophone          | TI significantly boosts CLAP similarity     |
| afrobeat  | An energetic `<afrobeat>` groove with drums      | TI outperforms baseline in genre fidelity   |




How It Works
	â€¢	Text prompts are passed through T5 encoder (frozen)
	â€¢	Prompt embeddings go into the MusicGen LM
	â€¢	The LM outputs codebook token IDs representing audio
	â€¢	Encodec decoder reconstructs the waveform
	â€¢	Our learned TI tokens are injected into the LM embedding space, not into the text encoder



Notes
	â€¢	Uses MusicGen-Large (4 codebooks, 2048-dim embeddings)
	â€¢	Encodec is used only during training and waveform decoding
	â€¢	TI tokens are not recognized by the text encoder



### Citation

If this project helps your work, please consider citing MusicGen:

@article{copet2023simple,
  title={Simple and Controllable Music Generation},
  author={Copet, Jade and DÃ©fossez, Alexandre and Plantinga, Peter and others},
  journal={arXiv preprint arXiv:2306.05284},
  year={2023}
}